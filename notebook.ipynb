{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klik op onderstaande knop (open in Colab) om de notebook te openen. Daarna kun je in Colab het installatiescript draaien. Hiervoor heb je een Google account nodig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/brunoruns/pix_llm/blob/main/notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installatiescript\n",
    "Klik op onderstaande cel om alle te installeren wat we vandaag nodig hebben. Dit wordt NIET geÃ¯nstalleerd op je computer, maar op een computer in de cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!wget -O environment.yml \"https://github.com/brunoruns/pix_llm/blob/main/environment.yml\"\n",
    "\n",
    "env_file = \"environment.yml\"\n",
    "if not os.path.exists('/usr/local/bin/conda'):\n",
    "    print(\"Installing Conda...\")\n",
    "    !wget -c https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "    !bash miniconda.sh -b -f -p /usr/local\n",
    "    !rm miniconda.sh\n",
    "    !/usr/local/bin/conda init\n",
    "\n",
    "# Activate conda and install packages from the environment file\n",
    "!eval \"$(conda shell.bash hook)\" && conda env create -f {env_file} -p /usr/local/env\n",
    "# Activate the environment and configure Jupyter to use it\n",
    "!eval \"$(conda shell.bash hook)\" && conda activate /usr/local/env && python -m ipykernel install --user --name=colab_env --display-name \"Python (colab_env)\"\n",
    "\n",
    "# Restart kernel\n",
    "import IPython\n",
    "IPython.display.clear_output()\n",
    "\n",
    "print(\"Conda environment installed and ready! Please select 'Python (colab_env)' as the kernel if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructie\n",
    "\n",
    "Je kan deze notebook gebruiken door met je cursus in een cel te gaan staan, en dan SHIFT-ENTER te typen. De code in die cel wordt dan uitgevoerd. Wat je ziet is python code, een populaire programmeertaal om met datastructure en AI te werken. De tussenresultaten van de code (de variabelen) worden zolang deze notebook open staat bewaard. Dit wil zeggen dat wanneer je achtereenvolgens volgende cellen gaat uitvoeren, er uiteindelijk 8 zal worden geprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "Bag-of-Words (BoW) is een eenvoudige techniek om tekst om te zetten in numerieke vorm, maar het heeft een aantal beperkingen, vooral bij het vastleggen van betekenis en context. Bij de one-hot encoding representeren we elk woord als een unieke vector met alleen nullen en een enkele 1 op de positie die overeenkomt met dat woord in de woordenschat.\n",
    "\n",
    "Bijvoorbeeld, stel we hebben de volgende woordenlijst (vocabulaire):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"koning\", \"koningin\", \"man\", \"vrouw\", \"troon\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dan zou de one-hot encoding er als volgt uitzien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "koning    = [1, 0, 0, 0, 0]  \n",
    "koningin  = [0, 1, 0, 0, 0]  \n",
    "man       = [0, 0, 1, 0, 0]  \n",
    "vrouw     = [0, 0, 0, 1, 0]  \n",
    "troon     = [0, 0, 0, 0, 1]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het probleem met deze manier van werken is dat er geen context of betekenis aan woorden kunnen worden meegegeven. Koning en koningin staan qua betekenis dicht bij elkaar, maar dat is uit deze representatie niet duidelijk. Bovendien heb je even lange vectoren nodig als het aantal woorden in je woordenlijst: dat is niet schaalbaar voor echte talen. Het Algemeen Nederlands Woordenboek (https://anw.ivdnt.org/search) bevat bijvoorbeeld meer dan 290 000 woorden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Word2Vec lost deze problemen op door woordvectoren te leren die in een lagere dimensie liggen (bijv. 300 getallen i.p.v. 100.000) Ã©n die betekenisvolle relaties tussen woorden behouden. Elk woord wordt dan 'ingebed' in een 300-dimensionele vectorruimte, vandaar de naam 'embedding'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de code hieronder laadt bestaande software in uit zogenaamde software bibliotheken (libraries)\n",
    "import gensim.downloader as api # dit gaan we gebruiken om een groot stuk voorbeeldtekst te downloaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = api.load('text8')  # download het corpus en slaat dit op in een variable corpus als een lijst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit bevat data, grotendeels afkomstig van Wikipedia. De eerste paar items van dit corpus zien er als volgt uit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Het corpus bestaat uit een lijst, die zelf telkens bestaat uit meerdere lijsten (lijst in lijst). Met de len-functie en een FOR-loop kun je opvragen hoeveel items er in deze lijsten zitten, door over elk van de lijstjes in de grote lijst te lopen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "grote_lijst = list(corpus)\n",
    "total_words = 0\n",
    "for kleine_lijst in grote_lijst:\n",
    "    print(\"Het eerste woord van deze lijst is \", kleine_lijst[0], \", het aantal woorden in deze lijst is \", len(kleine_lijst))\n",
    "    total_words = total_words + len(kleine_lijst)\n",
    "print(\"Het totaal aantal woorden in dit corpus is \", total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We weten nu hoeveel woorden er in totaal in deze tekst zitten. Dit geeft een idee van de grootte van deze dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probleemstelling\n",
    "\n",
    "We willen idealiter een vector die bepaalde aspecten van een woord kan capteren. Bekijk volgend fictief voorbeeld:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Source:https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673](https://miro.medium.com/v2/resize:fit:720/format:webp/1*Z-EOcLtUDlhvVuaruyM8EQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links staan kenmerken zoals 'koninklijkheid', 'mannelijkheid', 'vrouwelijkheid' en 'leeftijd'. Op die manier zouden we voor de 5 voorbeeldwoorden in de kolommen getallen kunnen kleven die 'coderen' in welke mate dat woord die kenmerken heeft. Wij kunnen dat omdat wij de taal spreken, een computer moet dit doen via een omweg. Een computer kijkt naar welke woorden in een bepaalde zin 'inpassen'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de zin \"de ____ zit op de troon\" passen typisch een aantal woorden:\n",
    "- koning\n",
    "- koningin\n",
    "- prins\n",
    "\n",
    "Deze zin geeft dus een context waar sommige woorden in passen, en andere niet. Word2Vec buit dit idee uit, door voor een gegeven tekst, de context rond elk woord te bekijken (de woorden vlak ervoor en vlak erna) en te kijken welke woorden in dezelfde contexten kunnen voorkomen. We hebben hierboven een groot corpus ingeladen, en dat gaan we gebruiken om het model te trainen - het model gaat op basis van die tekst zelf een zo goed mogelijke inbedding creÃ«ren.\n",
    "\n",
    "Je zou nu alle combinaties van woorden als verschillende contexten kunnen maken en daar een tabel van laten opstellen. Maar zelfs met een computer, is dat veel te veel werk, omdat het aantal mogelijke combinaties enorm snel stijgt. Er wordt iets slimmer gedaan, maar we zullen het model eerst gebruiken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec is een model dat in 2013 werd ontwikkeld door onderzoekers van Google\n",
    "from gensim.models.word2vec import Word2Vec # dit bevat een gekende embedder\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=5, min_count=1, workers=4) # op deze stap wordt het model ingeladen Ã©n getrained op het corpus. Daarom duurt dit wel even."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu het model 'geleerd' heeft wat het moet doen, kan je aan het model vragen om van een woord de vector-embedding weer te geven. Zoals verwacht, is dit een vector met lengte 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toon embedding van een woord\n",
    "print(model.wv[\"computer\"])  # Vector van het woord \"computer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wat volgt kan je nu 'met woorden rekenen'. We zullen een specifieke berekening doen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_vector = model.wv[\"king\"]\n",
    "man_vector = model.wv[\"man\"]\n",
    "woman_vector = model.wv[\"woman\"]\n",
    "queen_vector = model.wv['queen']\n",
    "\n",
    "new_vector = king_vector - man_vector + woman_vector\n",
    "print(new_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zo dadelijk zullen we zien dat deze vector, hoewel hij op het eerste zicht helemaal niet begrijpelijk lijkt, goed geslaagd is in zijn opzet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word Embedding Analogy](https://www.researchgate.net/profile/Jayaraman-J-Thiagarajan-2/publication/319370400/figure/fig1/AS:639487218548736@1529477042252/n-the-word-embedding-space-the-analogy-pairs-exhibit-interesting-algebraic-relationships.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hoe is dit model gemaakt ?\n",
    "Je kan dit model voorstel als een opeenvolging van input naar output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1_tmyks7pjdwxODh5-gL3FHQ.webp](https://miro.medium.com/v2/resize:fit:720/format:webp/1*XBaqjqpnBIXtXLzWjkuLQQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als input is elk woord gecodeerd volgens zijn positie in de tekst. De oranje cellen hierboven spelen elk de rol van een dimensie van de uiteindelijke vectorruimte waarnaar we de data willen omzetten. In dit geval 300. De output cellen in rood, geven aan of dit woord overeenkomt met een aantal op voorhand gedefinieerde woorden. Dit is een voorbeeld van een neuraal netwerk.\n",
    "\n",
    "Link naar een voorbeeld: https://playground.tensorflow.org \n",
    "\n",
    "Tussen elke laag in een neuraal netwerk zijn er pijltjes die aangeven waar elke output opnieuw als input wordt gebruikt. Elk van deze pijltjes heeft een bepaald gewicht, dat het neuraal network zelf moet leren. Vergelijk het met een pad in je brein waarin bepaalde neuronen sterker worden geactiveerd bij bepaalde input. Tussen de oranje en de rode laag wordt een softmax classifier gebruikt; die numerieke waarden omzet in kansen. Zo wordt een input vector omgezet in een reeks output waardes, waarbij de output waardes aangeven hoe groot de kans is dat een willekeurig gekozen woord in de buurt komt van een ander woord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om te begrijpen hoe dit werkt, kijken we naar een heel kleine versie van een neuraal netwerk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_network():\n",
    "    \"\"\"\n",
    "    Teken een diagram van een klein neuraal netwerk met:\n",
    "    - 2 input neuronen\n",
    "    - 4 neuronen in verborgen laag 1\n",
    "    - 3 neuronen in verborgen laag 2\n",
    "    - 1 output neuron\n",
    "    \"\"\"\n",
    "    # ðŸ”¹ Netwerkstructuur\n",
    "    layers = [2, 4, 3, 1]  # Aantal neuronen per laag\n",
    "    layer_positions = [0, 1, 2, 3]  # X-posities per laag\n",
    "    \n",
    "    # ðŸ”¹ Maak een lege graaf (graph)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # ðŸ”¹ Posities van de neuronen bepalen\n",
    "    pos = {}  \n",
    "    y_offsets = {}  # Houd bij hoeveel neuronen er al zijn in een laag\n",
    "\n",
    "    for layer_idx, num_nodes in enumerate(layers):\n",
    "        y_offset = -(num_nodes - 1) / 2  # Zorgt ervoor dat neuronen netjes verdeeld zijn\n",
    "        \n",
    "        for node_idx in range(num_nodes):\n",
    "            node_name = f\"L{layer_idx}_N{node_idx}\"  # Unieke naam per neuron\n",
    "            G.add_node(node_name, layer=layer_idx)  # Voeg neuron toe\n",
    "            pos[node_name] = (layer_positions[layer_idx], y_offset + node_idx)  # Bepaal positie\n",
    "\n",
    "    # ðŸ”¹ Verbind neuronen tussen de lagen\n",
    "    for layer_idx in range(len(layers) - 1):\n",
    "        for src_idx in range(layers[layer_idx]):\n",
    "            for dest_idx in range(layers[layer_idx + 1]):\n",
    "                src_node = f\"L{layer_idx}_N{src_idx}\"\n",
    "                dest_node = f\"L{layer_idx + 1}_N{dest_idx}\"\n",
    "                G.add_edge(src_node, dest_node)  # Voeg verbinding toe\n",
    "\n",
    "    # ðŸ”¹ Visualisatie van het neurale netwerk\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=False, node_size=800, node_color=\"skyblue\", edge_color=\"gray\", width=1.5, arrows=False)\n",
    "\n",
    "    # Labels per laag\n",
    "    for layer_idx, num_nodes in enumerate(layers):\n",
    "        x_pos = layer_positions[layer_idx]\n",
    "        y_pos = sum([pos[f\"L{layer_idx}_N{i}\"][1] for i in range(num_nodes)]) / num_nodes\n",
    "        label = \"Input\" if layer_idx == 0 else (\"Output\" if layer_idx == len(layers) - 1 else f\"Hidden Layer {layer_idx}\")\n",
    "        plt.text(x_pos, y_pos + 0.8, label, fontsize=12, ha=\"center\", color=\"black\", fontweight=\"bold\")\n",
    "\n",
    "    plt.title(\"Small Neural Netwerk Layout\")\n",
    "    plt.show()\n",
    "\n",
    "# ðŸš€ Genereer en toon de netwerkvisualisatie\n",
    "draw_neural_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# activatie functies\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)  # ReLU activatie\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # Sigmoid activatie\n",
    "\n",
    "def id(x): # dit is de identiteitsfunctie, die 'doet niks'\n",
    "    return x\n",
    "\n",
    "# Blauwdruk van een klein neural netwerk\n",
    "class SmallNeuralNetwork:\n",
    "    def __init__(self):\n",
    "        np.random.seed(42)  # om te zorgen dat de willekeurige getallen identiek blijven\n",
    "        \n",
    "        # gewichten en biases\n",
    "        self.W1 = np.random.randn(2, 4)  # Input Layer (2) â†’ Hidden Layer 1 (4)\n",
    "        self.b1 = np.random.randn(4)\n",
    "\n",
    "        self.W2 = np.random.randn(4, 3)  # Hidden Layer 1 (4) â†’ Hidden Layer 2 (3)\n",
    "        self.b2 = np.random.randn(3)\n",
    "\n",
    "        self.W3 = np.random.randn(3, 1)  # Hidden Layer 2 (3) â†’ Output Layer (1)\n",
    "        self.b3 = np.random.randn(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inferentie uitvoeren\n",
    "        :param x: Input vector of shape (2,)\n",
    "        :return: Output scalar (prediction)\n",
    "        \"\"\"\n",
    "        # Ensure input is 2D (batch size 1)\n",
    "        x = x.reshape(1, -1)\n",
    "\n",
    "        # ðŸš€ Forward Propagation: Matrix Multiplication\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1  # Input â†’ Hidden 1\n",
    "        self.a1 = relu(self.z1)                 # Apply ReLU\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Hidden 1 â†’ Hidden 2\n",
    "        self.a2 = relu(self.z2)                       # Apply ReLU\n",
    "        \n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3  # Hidden 2 â†’ Output\n",
    "        output = sigmoid(self.z3)                     # Apply Sigmoid\n",
    "        \n",
    "        return output.squeeze()  # Convert to scalar\n",
    "\n",
    "# Initialisatie\n",
    "nn = SmallNeuralNetwork()\n",
    "\n",
    "# Voorbeeldgebruik\n",
    "input_vector = np.array([0.5, -1.5])\n",
    "\n",
    "# ðŸ” Run the Forward Pass\n",
    "output = nn.forward(input_vector)\n",
    "\n",
    "# Resultaten printen\n",
    "print(\"Input Vector:\", input_vector)\n",
    "print(\"Predicted Output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan zo'n netwerk trainen door een *loss-function* op te stellen. Dat is een functie die 'meet' in welke mate de output van je netwerk aan bepaalde eigenschappen voldoet. Voor bovenstaan netwerk is dit een relalistische loss-functie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true, y_pred):\n",
    "    return -(y_true / (y_pred + 1e-8)) + ((1 - y_true) / (1 - y_pred + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je kan dan, als je bepaalde voorbeelden hebt waarvan je de gewenste uitkomst kent, volgende strategie volgen:\n",
    "- een voorbeeldvector invoeren\n",
    "- de voorspelde uitkomst (y_predicted) en de gekende uitkomst van het voorbeeld (y_true) vergelijken met de binary_cross_entropy\n",
    "- in de functie binary_cross_entropy, kijken in welke richting de functie een betere score heeft\n",
    "- de gewichtjes van het neuraal netwerk aanpassen zodat de voorspelling dichter bij de voorbeelduitkomst zit\n",
    "\n",
    "Dit proces heet backtracking. Centraal in het algoritme zit een techniek (gradient descent) die de afgeleide van de functies nodig heeft om te kunnen bepalen in welke richting de gewichtjes moeten worden aangepast. Voor de volledigheid geef ik hieronder een implentatie van hoe het model hierboven dan eerst kan worden getraind, als je voorbeeldinformatie hebt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ðŸš€ Activatiefuncties en hun afgeleiden\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)  # ReLU activatie: negatieve waarden worden 0\n",
    "\n",
    "def relu_afgeleide(x):\n",
    "    return (x > 0).astype(float)  # Afgeleide van ReLU: 1 als x > 0, anders 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))  # Sigmoid activatie: output tussen 0 en 1\n",
    "\n",
    "def sigmoid_afgeleide(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)  # Afgeleide van sigmoid\n",
    "\n",
    "# Loss functie: Binair Cross-Entropy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "def binary_cross_entropy_afgeleide(y_true, y_pred):\n",
    "    return -(y_true / (y_pred + 1e-8)) + ((1 - y_true) / (1 - y_pred + 1e-8))\n",
    "\n",
    "# Klein neuraal netwerk met backpropagation\n",
    "class KleinNeuraalNetwerk:\n",
    "    def __init__(self, leer_snelheid=0.05):\n",
    "        np.random.seed(42)  # Zorgen voor reproduceerbare resultaten\n",
    "        self.lr = leer_snelheid  # Leersnelheid voor gradient descent\n",
    "        \n",
    "        # Initialisatie van gewichten en biases\n",
    "        self.W1 = np.random.randn(2, 4) * 0.01 # Input â†’ Verborgen laag 1 (4 neuronen)\n",
    "        self.b1 = np.zeros(4)\n",
    "\n",
    "        self.W2 = np.random.randn(4, 3) *0.01 # Verborgen laag 1 â†’ Verborgen laag 2 (3 neuronen)\n",
    "        self.b2 = np.zeros(3)\n",
    "\n",
    "        self.W3 = np.random.randn(3, 1) * 0.01 # Verborgen laag 2 â†’ Output (1 neuron)\n",
    "        self.b3 = np.zeros(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Voer de forward pass uit met matrixvermenigvuldiging.\n",
    "        :param x: Input vector met vorm (2,)\n",
    "        :return: Output waarde (voorspelling)\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)  # Zorgen dat input tweedimensionaal is\n",
    "        \n",
    "        # ðŸš€ Forward propagatie: matrixvermenigvuldiging en activaties\n",
    "        self.z1 = np.dot(x, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)  # Toepassen van ReLU\n",
    "        \n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = relu(self.z2)  # Toepassen van ReLU\n",
    "        \n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = sigmoid(self.z3)  # Toepassen van sigmoid (laatste output)\n",
    "\n",
    "        return self.a3.squeeze()  # Omzetten naar een scalaire waarde\n",
    "\n",
    "    def backward(self, x, y_true):\n",
    "        \"\"\"\n",
    "        Voer backpropagation uit om gewichten te updaten.\n",
    "        :param x: Input vector (2,)\n",
    "        :param y_true: Werkelijke labelwaarde (0 of 1)\n",
    "        \"\"\"\n",
    "        x = x.reshape(1, -1)  # Zorgen dat input tweedimensionaal is\n",
    "        \n",
    "        # ðŸš€ Forward pass opnieuw uitvoeren om de activaties te berekenen\n",
    "        self.forward(x)\n",
    "\n",
    "        # ðŸš€ Berekening van de afgeleide van de verliesfunctie\n",
    "        dL_dz3 = binary_cross_entropy_afgeleide(y_true, self.a3) * sigmoid_afgeleide(self.z3)\n",
    "        #dL_dz3 = (self.a3 - y_true)\n",
    "\n",
    "        # ðŸš€ Backpropagation door de lagen\n",
    "        dL_dW3 = np.dot(self.a2.T, dL_dz3)\n",
    "        dL_db3 = dL_dz3\n",
    "\n",
    "        dL_dz2 = np.dot(dL_dz3, self.W3.T) * relu_afgeleide(self.z2)\n",
    "        dL_dW2 = np.dot(self.a1.T, dL_dz2)\n",
    "        dL_db2 = dL_dz2\n",
    "\n",
    "        dL_dz1 = np.dot(dL_dz2, self.W2.T) * relu_afgeleide(self.z1)\n",
    "        dL_dW1 = np.dot(x.T, dL_dz1)\n",
    "        dL_db1 = dL_dz1\n",
    "\n",
    "        # ðŸš€ Gewichten en biases updaten met gradient descent\n",
    "        self.W3 -= self.lr * dL_dW3\n",
    "        self.b3 -= self.lr * np.sum(dL_db3, axis=0)\n",
    "\n",
    "        self.W2 -= self.lr * dL_dW2\n",
    "        self.b2 -= self.lr * np.sum(dL_db2, axis=0)\n",
    "\n",
    "        self.W1 -= self.lr * dL_dW1\n",
    "        self.b1 -= self.lr * np.sum(dL_db1, axis=0)\n",
    "\n",
    "    def train(self, X, y, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train het netwerk met backpropagation.\n",
    "        :param X: Trainingsdata (N x 2)\n",
    "        :param y: Doellabels (N x 1)\n",
    "        :param epochs: Aantal iteraties\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            verlies = 0\n",
    "            for i in range(len(X)):\n",
    "                self.backward(X[i], y[i])  # Backpropagation uitvoeren\n",
    "                verlies += binary_cross_entropy(y[i], self.forward(X[i]))  # Verlies berekenen\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Verlies: {verlies / len(X):.4f}\")\n",
    "\n",
    "# Het neurale netwerk trainen\n",
    "nn = KleinNeuraalNetwerk(leer_snelheid=0.1)\n",
    "\n",
    "# Voorbeeld trainingsdata (XOR-probleem)\n",
    "X_train = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 1, 1, 0])  # Binaire labels\n",
    "\n",
    "# Train het netwerk\n",
    "nn.train(X_train, y_train, epochs=1000)\n",
    "\n",
    "# Voorspellingen testen\n",
    "for x in X_train:\n",
    "    print(f\"Input: {x}, Voorspelling: {nn.forward(x):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toegepast op het Word2Vec neuraal netwerk: je kan, als je een hele tekst hebt, de woorden en hun context stukje per stukje aan dit model geven, en de gewichtjes laten verbeteren zodanig dat uiteindelijk dit model zo goed mogelijk deze waardes benaderd.\n",
    "- We hebben namelijk veel voorbeelden van woorden en de omliggende woorden die voorspeld zouden moeten worden\n",
    "- we kunnen dus dit model *trainen*: we geven achtereenvolgens een woord en de verwachte uitkomst, en laten het model de gewichtjes aanpassen om zo dicht mogelijk bij het eindresultaat te komen.\n",
    "- op die manier gaat de computer zÃ©lf leren welke informatie er in die vectorrepresentatie wordt bijgehouden, dit zal voor een mens niet meer zo makkelijk interpreteerbaar zijn als \"koninklijkheid\" of \"leeftijd\" zoals voordien.\n",
    "\n",
    "Het enige wat daarbij nog extra kan worden opgemerkt is dat je in het geval van een taalmodel nooit 1 enkele oplossing (woord) hebt voor een bepaalde context (een gemaskeerde zin). Daarom train je het model om een kanstheoretische verdeling als output te geven, die aangeeft wat de kans is dat je een andere woord uit het corpus daar in de buurt tegenkomt. Als je dat op een heel kleine schaal bekijkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# ðŸš€ 1. Voorbeeldcorpus: Eenvoudige zinnen met context\n",
    "corpus_klein = [\n",
    "    [\"hond\", \"blaffen\", \"kat\", \"miauwen\", \"aaien\"],\n",
    "    [\"kat\", \"aaien\", \"zacht\", \"vriendelijk\"],\n",
    "    [\"hond\", \"rennen\", \"spelen\", \"bos\"],\n",
    "    [\"kat\", \"spinnen\", \"slaap\", \"bank\"],\n",
    "    [\"aaien\", \"zacht\", \"liefdevol\", \"kat\"],\n",
    "    [\"kind\", \"aaien\", \"kat\", \"vriendelijk\"]\n",
    "]\n",
    "\n",
    "# Train een Word2Vec-model met 5 dimensies\n",
    "model_klein = Word2Vec(sentences=corpus_klein, vector_size=2, window=2, min_count=1, workers=4)\n",
    "\n",
    "# Visualiseer de woordvectoren in 2D\n",
    "words = [\"kat\", \"aaien\", \"hond\", \"blaffen\", \"zacht\", \"vriendelijk\"]\n",
    "word_vectors = np.array([model_klein.wv[word] for word in words])\n",
    "\n",
    "# Plot de woordvectoren\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(word_vectors[:, 0], word_vectors[:, 1])\n",
    "for word, (x, y) in zip(words, word_vectors):\n",
    "    plt.text(x, y, word, fontsize=12)\n",
    "plt.title(\"Woordvectoren in 2D\")\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![source:](https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=2048&disable=upscale&auto=webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wat kun je met het getraind model doen ?\n",
    "Het getraind model heeft *(op magische wijze!)* een vorm van betekenis geleerd: woorden die 'gelijkaardig' zijn in betekenis, liggen 'dicht bij elkaar. Dat dicht bij elkaar kunnen we uitrekenen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = model.wv.similarity('computer', 'graphics')\n",
    "print(similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bovendien komt 'dicht bij mekaar' echt overeen met wat je in 2D of 3D al kent voor meetkundige hoeken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vec_col = new_vector.reshape(1,-1) # wisselen van rijvector naar kolomvector\n",
    "queen_vec_col = queen_vector.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_score = cosine_similarity(new_vec_col, queen_vec_col)\n",
    "similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2) # dit vermenigvuldigt elementsgewijs en telt alles daarna op. Geeft een scalair als resultaat\n",
    "    \n",
    "    # de vectoren moeten worden genormeerd\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Similarity\n",
    "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "\n",
    "# je eigen functie gebruiken:\n",
    "similarity_score = cosine_similarity(new_vector, queen_vector)\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity_score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cosine similarity](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GK56xmDIWtNQAD_jnBIt2g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "Deep learning models hadden op zich nog niet genoeg kracht om vlot tekst te kunnen verwerken. Het zou duren tot een nieuwe innovatie in 2016 hier verandering in bracht, door in de architectuur van een neuraal netwerk een vorm van geheugen in te bouwen. Dit noemen we 'attention'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional awareness\n",
    "\n",
    "In de code hieronder wordt getoond hoe je de positie van een woord in een zin kan coderen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_small = Word2Vec(sentences=corpus, vector_size=10, window=4, min_count=1, workers=4)\n",
    "sentence = [\"the\", \"queen\", \"does\", \"not\", \"like\", \"art\", 'but', 'the', 'king', 'does']\n",
    "embedding_dim = model_small.vector_size\n",
    "print(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = torch.tensor([model_small.wv[word] for word in sentence])  # Shape: (sequence_length, embedding_dim)\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add batch dimension: (1, sequence_length, embedding_dim)\n",
    "input_sequence = word_vectors.unsqueeze(0)\n",
    "print(input_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieronder een simpel neuraal netwerk, zonder woordvolgorde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpel  Neural Network (zonder positie)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)  # Transformeer input embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "simple_nn = SimpleNN(embedding_dim=embedding_dim)\n",
    "output_simple = simple_nn(input_sequence)\n",
    "print(\"Simple Neural Network Output (heeft geen notie van positie):\\n\", output_simple.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer met positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Matrix met shape (vorm) (max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # Shape (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Oneven indices\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)  # Shape (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "# Transformer-achtig Model met positional encoding\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)  # Transformeer embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate model\n",
    "transformer = TransformerModel(d_model=embedding_dim)\n",
    "\n",
    "# input door model halen\n",
    "output_transformer = transformer(input_sequence)\n",
    "\n",
    "print(\"Simple Neural Network Output (heeft geen notie van positie):\\n\", output_simple.detach().numpy())\n",
    "print(\"\\nTransformer met positional encoding output:\\n\", output_transformer.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisatie van het verschil\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(transformer.positional_encoding.pe.squeeze().numpy(), cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title(\"Positional Encoding Values\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Woord positie\")\n",
    "\n",
    "plt.yticks(ticks=range(len(sentence)), labels=sentence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergelijken van 'gelijkaardige' zinnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"on\", \"the\", \"mat\", \"sat\", \"the\", \"cat\"],\n",
    "    [\"the\", \"mat\", \"was\", \"sat\", \"on\", \"by\", \"the\", \"cat\"],\n",
    "    [\"sat\", \"on\", \"the\", \"mat\", \"the\", \"cat\", \"did\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = model_small.vector_size\n",
    "word_vectors = [torch.tensor([model_small.wv[word] for word in sentence if word in model_small.wv]) for sentence in sentences]\n",
    "word_vectors = [wv.unsqueeze(0) for wv in word_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerWithAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(TransformerWithAttention, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=2, batch_first=True)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        attn_output, _ = self.attention(x, x, x)  # Self-attention\n",
    "        return self.fc(attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ðŸš€ Functie om beide modellen in Ã©Ã©n plot te visualiseren\n",
    "def plot_sentence_comparison(sentence, simple_embedding, transformer_embedding, sentence_index):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # 1 rij, 2 kolommen\n",
    "\n",
    "    # Plot voor het simpele model (zonder attention)\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(simple_embedding, cmap='viridis', aspect='auto')\n",
    "    ax1.set_title(f\"Simple NN Output (zonder Attention) - Sentence {sentence_index}\")\n",
    "    ax1.set_xlabel(\"Embedding Dimension\")\n",
    "    ax1.set_ylabel(\"Word Position\")\n",
    "    ax1.set_yticks(range(len(sentence)))\n",
    "    ax1.set_yticklabels(sentence)\n",
    "    fig.colorbar(im1, ax=ax1)\n",
    "\n",
    "    # Plot voor het transformer model (met attention)\n",
    "    ax2 = axes[1]\n",
    "    im2 = ax2.imshow(transformer_embedding, cmap='viridis', aspect='auto')\n",
    "    ax2.set_title(f\"Transformer Output (met Attention) - Sentence {sentence_index}\")\n",
    "    ax2.set_xlabel(\"Embedding Dimension\")\n",
    "    ax2.set_yticks(range(len(sentence)))\n",
    "    ax2.set_yticklabels(sentence)\n",
    "    fig.colorbar(im2, ax=ax2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ðŸš€ Itereer over de zinnen en maak gecombineerde plots\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"\\nSentence variatie  {i+1}: {' '.join(sentence)}\")\n",
    "\n",
    "    # Visualisatie van beide modellen in Ã©Ã©n plot\n",
    "    plot_sentence_comparison(\n",
    "        sentence,\n",
    "        outputs_simple_np[i],\n",
    "        outputs_transformer_np[i],\n",
    "        sentence_index=i+1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voorbij transformers naar GPT\n",
    "\n",
    "Nu we heel snel hebben uitgelegd wat een transformer is, kunnen we kijken naar GPT modellen, zoals GPT-2. GPT staat voor Generative Pre-trained Transformer, waarvan we de laatste 2 woorden al hebben verklaard:\n",
    "- Het is een model dat getrained is (op publieke tekst, beschikbaar op het inernet)\n",
    "- Het is een transformer model, dwz het is een deep neural network met een bepaalde architectuur zodat het belang kan hechten aan woordvolgorde.\n",
    "\n",
    "Het woord 'generative' betekent dat dit model tijdens het trainen geleerd heeft om in bepaalde stukken tekst een woord aan te vullen, zoals 'De kat krabt de krollen van de ___'. Hierdoor kan dit model, als je het een stuk tekst geeft, het meest waarschijnlijke volgende woord aangeven.\n",
    "\n",
    "GPT2 is een model dat werd ontwikkeld in 2019, en getraind werd op 8 miljoen webpagina's. We gebruiken hier de kleine versie met 114 miljoen (114 000 000) parameters. Dit wil grosso modo zeggen dat er in het neuraal netwerk 114 000 000 gewichtjes op punt moesten worden gesteld om deze machine te laten werken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# We laden dit model in, dit wil zeggen: dit is een versie waar de gewichtjes voor alle duidelijkheid al vast staan (die berekening, het trainen, gebeurde eerder: pre-trained)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierbij een zin\n",
    "input_text = \"We are in a classroom. Artificial Intelligence is changing the way we \"\n",
    "\n",
    "# Volgende woorden genereren\n",
    "output = generator(input_text, max_length=20, num_return_sequences=1)\n",
    "\n",
    "# Print the predicted continuation\n",
    "print(\"Voorspelde tekst:\\n\", output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met een model kunnen we ook, naast de meest waarschijnlijke voorspelling, kijken naar net iets minder waarschijnlijke woorden. Dit kunnen je regelen met de zogenaamde 'temperatuur' instelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(input_text, max_length=20, num_return_sequences=3, temperature=0.8)\n",
    "for i, result in enumerate(output):\n",
    "    print(f\"Prediction {i+1}: {result['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat de data waarop dit model is getrained, ongefilterde is vanop het internet, geeft de maker van deze dataset volgende waarschuwing:\n",
    "\n",
    "\n",
    "*\"Because large-scale language models like GPT-2 do not distinguish fact from fiction, **we donâ€™t support use-cases that require the generated text to be true**.*\n",
    "*Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM's\n",
    "\n",
    "Larga language models zijn dus grote modellen die je kan gebruiken om tekst te voorspellen. Je kan zelf zulke getrainde modellen downloaden en op je eigen computer draaien. Je zal merken dat deze modellen veel trager reageren dan je gewend bent van bijvoorbeeld chatGPT. Dit is omdat jouw computer moeite heeft met de mijarden berekeningen die het voor elk woord te voorspellen, moet doen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tekst genereren\n",
    "\n",
    "Wanneer je 1 token kan voorspellen, is het een koud kunstje voor een computer om een heel antwoord te maken: je geeft bij een prompt door een gebruiker volgende tekst mee:\n",
    "\n",
    "\"\n",
    "USER: \"Hallo, chatGPT, kun je me iets vertellen over Napoleon?\"\n",
    "SYSTEM: \"Ja, Napoleon is een Franse \"\n",
    "\"\n",
    "\n",
    "Bij elk volgend woord wordt deze input die je meegeeft aan de LLM iets langer, en op die manier produceert de machine tekst. Door de tekst van de gebruiker en die van de machine zelf duidelijk te coderen, krijg je iets wat op een conversatie lijkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langs een API, naar een model op internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# environment vars inladen\n",
    "load_dotenv('secrets.env')\n",
    "\n",
    "def ask_mistral_api(question):\n",
    "    api_token = os.getenv(\"MISTRAL_TOKEN\")\n",
    "    model = \"mistral-large-latest\"\n",
    "\n",
    "    client = Mistral(api_key=api_token)\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return chat_response\n",
    "\n",
    "# Example usage\n",
    "question = \"Wat is de hoofdstad van Frankrijk?\"\n",
    "answer = ask_mistral_api(question)\n",
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lokaal, via Ollama\n",
    "Als je deze demo zelf wil doen, zou je ollama moeten installeren op je systeem. Dit kan je thuis eens proberen ([www.ollama.ai](https://ollama.com/))\n",
    "het verschil met de vorige code is dat dit model niet meer langs het internet gaat, maar enkel modellen lokaal op jouw computer gebruikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De code hieronder zal beduidend langer duren dan de vorige, want je maakt nu geen gebruik meer van heel krachtige servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model='mistral', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Hoe werkt een regenboog?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pix-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
